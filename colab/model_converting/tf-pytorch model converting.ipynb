{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1nxKnUj9NTHw2QU2f4EiJ-5clM1yB5iof","authorship_tag":"ABX9TyO0iKaVR6Wr5MPPblqlb+fh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#BERT development praparation\n","BERT needs python3.6, tensorflow1.13.1, pytorch1.10.0, pytorch-transformers1.2.0 to run properly."],"metadata":{"id":"U3-jXGJtzL4A"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"KRadyDGiy7Vh","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1684821787690,"user_tz":-480,"elapsed":177013,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"84cc26ad-d63d-4ddf-8180-23ee52725f4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.11\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  libpython3.6-minimal libpython3.6-stdlib python3.6-minimal\n","Suggested packages:\n","  python3.6-venv binfmt-support\n","The following NEW packages will be installed:\n","  libpython3.6-minimal libpython3.6-stdlib python3.6 python3.6-minimal\n","0 upgraded, 4 newly installed, 0 to remove and 24 not upgraded.\n","Need to get 4,294 kB of archives.\n","After this operation, 22.1 MB of additional disk space will be used.\n","Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.6-minimal amd64 3.6.15-1+focal3 [569 kB]\n","Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-minimal amd64 3.6.15-1+focal3 [1,718 kB]\n","Get:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.6-stdlib amd64 3.6.15-1+focal3 [1,758 kB]\n","Get:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6 amd64 3.6.15-1+focal3 [248 kB]\n","Fetched 4,294 kB in 4s (1,011 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package libpython3.6-minimal:amd64.\n","(Reading database ... 122531 files and directories currently installed.)\n","Preparing to unpack .../libpython3.6-minimal_3.6.15-1+focal3_amd64.deb ...\n","Unpacking libpython3.6-minimal:amd64 (3.6.15-1+focal3) ...\n","Selecting previously unselected package python3.6-minimal.\n","Preparing to unpack .../python3.6-minimal_3.6.15-1+focal3_amd64.deb ...\n","Unpacking python3.6-minimal (3.6.15-1+focal3) ...\n","Selecting previously unselected package libpython3.6-stdlib:amd64.\n","Preparing to unpack .../libpython3.6-stdlib_3.6.15-1+focal3_amd64.deb ...\n","Unpacking libpython3.6-stdlib:amd64 (3.6.15-1+focal3) ...\n","Selecting previously unselected package python3.6.\n","Preparing to unpack .../python3.6_3.6.15-1+focal3_amd64.deb ...\n","Unpacking python3.6 (3.6.15-1+focal3) ...\n","Setting up libpython3.6-minimal:amd64 (3.6.15-1+focal3) ...\n","Setting up python3.6-minimal (3.6.15-1+focal3) ...\n","Setting up libpython3.6-stdlib:amd64 (3.6.15-1+focal3) ...\n","Setting up python3.6 (3.6.15-1+focal3) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Processing triggers for mime-support (3.64ubuntu1) ...\n","update-alternatives: using /usr/bin/python3.6 to provide /usr/bin/python3 (python3) in auto mode\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  python3.6-lib2to3\n","The following NEW packages will be installed:\n","  python3.6-distutils python3.6-lib2to3\n","0 upgraded, 2 newly installed, 0 to remove and 24 not upgraded.\n","Need to get 308 kB of archives.\n","After this operation, 1,232 kB of additional disk space will be used.\n","Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-lib2to3 all 3.6.15-1+focal3 [122 kB]\n","Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-distutils all 3.6.15-1+focal3 [187 kB]\n","Fetched 308 kB in 1s (225 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package python3.6-lib2to3.\n","(Reading database ... 123142 files and directories currently installed.)\n","Preparing to unpack .../python3.6-lib2to3_3.6.15-1+focal3_all.deb ...\n","Unpacking python3.6-lib2to3 (3.6.15-1+focal3) ...\n","Selecting previously unselected package python3.6-distutils.\n","Preparing to unpack .../python3.6-distutils_3.6.15-1+focal3_all.deb ...\n","Unpacking python3.6-distutils (3.6.15-1+focal3) ...\n","Setting up python3.6-lib2to3 (3.6.15-1+focal3) ...\n","Setting up python3.6-distutils (3.6.15-1+focal3) ...\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  python-pip-whl python3-setuptools python3-wheel\n","Suggested packages:\n","  python-setuptools-doc\n","The following NEW packages will be installed:\n","  python-pip-whl python3-pip python3-setuptools python3-wheel\n","0 upgraded, 4 newly installed, 0 to remove and 24 not upgraded.\n","Need to get 2,389 kB of archives.\n","After this operation, 4,933 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.8 [1,805 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3-setuptools all 45.2.0-1ubuntu0.1 [330 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.8 [231 kB]\n","Fetched 2,389 kB in 1s (3,125 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package python-pip-whl.\n","(Reading database ... 123281 files and directories currently installed.)\n","Preparing to unpack .../python-pip-whl_20.0.2-5ubuntu1.8_all.deb ...\n","Unpacking python-pip-whl (20.0.2-5ubuntu1.8) ...\n","Selecting previously unselected package python3-setuptools.\n","Preparing to unpack .../python3-setuptools_45.2.0-1ubuntu0.1_all.deb ...\n","Unpacking python3-setuptools (45.2.0-1ubuntu0.1) ...\n","Selecting previously unselected package python3-wheel.\n","Preparing to unpack .../python3-wheel_0.34.2-1ubuntu0.1_all.deb ...\n","Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...\n","Selecting previously unselected package python3-pip.\n","Preparing to unpack .../python3-pip_20.0.2-5ubuntu1.8_all.deb ...\n","Unpacking python3-pip (20.0.2-5ubuntu1.8) ...\n","Setting up python3-setuptools (45.2.0-1ubuntu0.1) ...\n","Setting up python3-wheel (0.34.2-1ubuntu0.1) ...\n","Setting up python-pip-whl (20.0.2-5ubuntu1.8) ...\n","Setting up python3-pip (20.0.2-5ubuntu1.8) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pip\n","  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 9.3 MB/s \n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 20.0.2\n","    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n","    Can't uninstall 'pip'. No files were found to uninstall.\n","Successfully installed pip-21.3.1\n","Python 3.6.15\n","\u001b[33mWARNING: Skipping six as it is not installed.\u001b[0m\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow==1.13.1\n","  Downloading tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5 MB)\n","     |████████████████████████████████| 92.5 MB 70 kB/s              \n","\u001b[?25hCollecting pytorch-transformers==1.2.0\n","  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n","     |████████████████████████████████| 176 kB 67.1 MB/s            \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==1.13.1) (0.34.2)\n","Collecting keras-preprocessing>=1.0.5\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","     |████████████████████████████████| 42 kB 1.3 MB/s             \n","\u001b[?25hCollecting keras-applications>=1.0.6\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","     |████████████████████████████████| 50 kB 7.0 MB/s             \n","\u001b[?25hCollecting six>=1.10.0\n","  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting tensorboard<1.14.0,>=1.13.0\n","  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n","     |████████████████████████████████| 3.2 MB 45.1 MB/s            \n","\u001b[?25hCollecting numpy>=1.13.3\n","  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n","     |████████████████████████████████| 14.8 MB 55.8 MB/s            \n","\u001b[?25hCollecting protobuf>=3.6.1\n","  Downloading protobuf-3.19.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","     |████████████████████████████████| 1.1 MB 68.1 MB/s            \n","\u001b[?25hCollecting astor>=0.6.0\n","  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n","Collecting grpcio>=1.8.6\n","  Downloading grpcio-1.48.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","     |████████████████████████████████| 4.6 MB 72.6 MB/s            \n","\u001b[?25hCollecting termcolor>=1.1.0\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gast>=0.2.0\n","  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n","Collecting absl-py>=0.1.6\n","  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n","     |████████████████████████████████| 126 kB 74.1 MB/s            \n","\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n","  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n","     |████████████████████████████████| 367 kB 78.9 MB/s            \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","     |████████████████████████████████| 1.3 MB 61.9 MB/s            \n","\u001b[?25hCollecting requests\n","  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n","     |████████████████████████████████| 63 kB 2.1 MB/s             \n","\u001b[?25hCollecting torch>=1.0.0\n","  Downloading torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n","     |████████████████████████████████| 881.9 MB 5.3 kB/s             \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","     |████████████████████████████████| 880 kB 43.4 MB/s            \n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting regex\n","  Downloading regex-2023.5.5-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (756 kB)\n","     |████████████████████████████████| 756 kB 44.9 MB/s            \n","\u001b[?25hCollecting tqdm\n","  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n","     |████████████████████████████████| 78 kB 6.1 MB/s             \n","\u001b[?25hCollecting boto3\n","  Downloading boto3-1.23.10-py3-none-any.whl (132 kB)\n","     |████████████████████████████████| 132 kB 72.6 MB/s            \n","\u001b[?25hCollecting h5py\n","  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n","     |████████████████████████████████| 4.0 MB 53.1 MB/s            \n","\u001b[?25hCollecting werkzeug>=0.11.15\n","  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n","     |████████████████████████████████| 289 kB 58.3 MB/s            \n","\u001b[?25hCollecting markdown>=2.6.8\n","  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n","     |████████████████████████████████| 97 kB 7.9 MB/s             \n","\u001b[?25hCollecting mock>=2.0.0\n","  Downloading mock-5.0.2-py3-none-any.whl (30 kB)\n","Collecting dataclasses\n","  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n","Collecting typing-extensions\n","  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n","Collecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n","     |████████████████████████████████| 79 kB 9.3 MB/s             \n","\u001b[?25hCollecting botocore<1.27.0,>=1.26.10\n","  Downloading botocore-1.26.10-py3-none-any.whl (8.8 MB)\n","     |████████████████████████████████| 8.8 MB 56.4 MB/s            \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting certifi>=2017.4.17\n","  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n","     |████████████████████████████████| 156 kB 79.1 MB/s            \n","\u001b[?25hCollecting idna<4,>=2.5\n","  Downloading idna-3.4-py3-none-any.whl (61 kB)\n","     |████████████████████████████████| 61 kB 115 kB/s             \n","\u001b[?25hCollecting charset-normalizer~=2.0.0\n","  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n","Collecting urllib3<1.27,>=1.21.1\n","  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n","     |████████████████████████████████| 140 kB 56.1 MB/s            \n","\u001b[?25hCollecting click\n","  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n","     |████████████████████████████████| 97 kB 7.8 MB/s             \n","\u001b[?25hCollecting joblib\n","  Downloading joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n","     |████████████████████████████████| 309 kB 57.1 MB/s            \n","\u001b[?25hCollecting importlib-resources\n","  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n","Collecting python-dateutil<3.0.0,>=2.1\n","  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n","     |████████████████████████████████| 247 kB 66.1 MB/s            \n","\u001b[?25hCollecting importlib-metadata>=4.4\n","  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n","Collecting cached-property\n","  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n","Collecting zipp>=3.1.0\n","  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n","Building wheels for collected packages: termcolor, sacremoses\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=96db480080b5dea60cab20db80d5acac3ce64ddf1ba22f81b93179ba24f9d188\n","  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895255 sha256=2da6d14b935f6c4b887b471f13c0c05a68a0f23eab344310a9a787d37ddd5495\n","  Stored in directory: /root/.cache/pip/wheels/4c/64/31/e9900a234b23fb3e9dc565d6114a9d6ff84a72dbdd356502b4\n","Successfully built termcolor sacremoses\n","Installing collected packages: six, zipp, urllib3, typing-extensions, python-dateutil, jmespath, numpy, importlib-resources, importlib-metadata, dataclasses, cached-property, botocore, werkzeug, tqdm, s3transfer, regex, protobuf, mock, markdown, joblib, idna, h5py, grpcio, click, charset-normalizer, certifi, absl-py, torch, termcolor, tensorflow-estimator, tensorboard, sentencepiece, sacremoses, requests, keras-preprocessing, keras-applications, gast, boto3, astor, tensorflow, pytorch-transformers\n","Successfully installed absl-py-1.4.0 astor-0.8.1 boto3-1.23.10 botocore-1.26.10 cached-property-1.5.2 certifi-2023.5.7 charset-normalizer-2.0.12 click-8.0.4 dataclasses-0.8 gast-0.5.4 grpcio-1.48.2 h5py-3.1.0 idna-3.4 importlib-metadata-4.8.3 importlib-resources-5.4.0 jmespath-0.10.0 joblib-1.1.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.7 mock-5.0.2 numpy-1.19.5 protobuf-3.19.6 python-dateutil-2.8.2 pytorch-transformers-1.2.0 regex-2023.5.5 requests-2.27.1 s3transfer-0.5.2 sacremoses-0.0.53 sentencepiece-0.1.99 six-1.16.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0 torch-1.10.2 tqdm-4.64.1 typing-extensions-4.1.1 urllib3-1.26.15 werkzeug-2.0.3 zipp-3.6.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["charset_normalizer","dateutil","requests","six","urllib3"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Python 3.6.15\n"]}],"source":["!python -V\n","!sudo apt-get install python3.6\n","!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 3\n","#!sudo update-alternatives --config python3\n","!sudo apt-get install python3.6-distutils\n","!sudo apt install python3-pip\n","!python -m pip install --upgrade pip\n","!python -V\n","!pip uninstall six -y\n","#!pip install tensorflow==1.11.0 pytorch-transformers==1.2.0\n","!pip install tensorflow==1.13.1 pytorch-transformers==1.2.0\n","!python -V"]},{"cell_type":"markdown","source":["Install Huawei OBSUtil to update converted pytorch model."],"metadata":{"id":"UizyzI3hzizw"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code\n","!wget https://obs-community-intl.obs.ap-southeast-1.myhuaweicloud.com/obsutil/current/obsutil_linux_amd64.tar.gz -P /content/drive/MyDrive/Code\n","!tar xvf obsutil_linux_amd64.tar.gz\n","!chmod 777 obsutil_linux_amd64_5.4.11/obsutil\n","!mv obsutil_linux_amd64_5.4.11/obsutil /bin/\n","!rm -rf /content/drive/MyDrive/Code/obsutil_linux_amd64_5.4.11"],"metadata":{"id":"cYlELJPazhBf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1.Download a BERT TF model. 2.Convert by pytorch_transformers library. 3.Tar outputs in a zip file and name it with a '_pytorch' surfix."],"metadata":{"id":"Zv4Li1r4zYsp"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code\n","!wget https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/chinese_L-12_H-768_A-12.tgz -P /content/drive/MyDrive/Code\n","!tar xvf chinese_L-12_H-768_A-12.tgz\n","%env BERT_BASE_DIR=chinese_L-12_H-768_A-12\n","\n","!pytorch_transformers bert \\\n","  $BERT_BASE_DIR/bert_model.ckpt \\\n","  $BERT_BASE_DIR/bert_config.json \\\n","  $BERT_BASE_DIR/pytorch_model.bin\n","\n","%env BERT_BASE_PYTORCH_FILE=chinese_L-12_H-768_A-12_pytorch.tgz\n","!tar cvzf $BERT_BASE_PYTORCH_FILE $BERT_BASE_DIR/bert_config.json $BERT_BASE_DIR/pytorch_model.bin $BERT_BASE_DIR/vocab.txt"],"metadata":{"id":"Rgc1INlezZV2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code_pretrain\n","%env BERT_BASE_DIR=chinese_L-2_H-128_A-2\n","!pytorch_transformers bert \\\n","  $BERT_BASE_DIR/bert_model.ckpt \\\n","  $BERT_BASE_DIR/bert_config.json \\\n","  $BERT_BASE_DIR/pytorch_model.bin\n","\n","# %env BERT_BASE_PYTORCH_FILE=chinese_L-2_H-128_A-2_pytorch.tgz\n","# !tar cvzf $BERT_BASE_PYTORCH_FILE $BERT_BASE_DIR/bert_config.json $BERT_BASE_DIR/pytorch_model.bin $BERT_BASE_DIR/vocab.txt"],"metadata":{"id":"iZ605c_ofO4a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Convertion of BERT-Mini\n"],"metadata":{"id":"BZjzDFOwKDuz"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code_pretrain\n","%env BERT_BASE_DIR=chinese_L-4_H-256_A-4\n","!pytorch_transformers bert \\\n","  $BERT_BASE_DIR/bert_model.ckpt \\\n","  $BERT_BASE_DIR/bert_config.json \\\n","  $BERT_BASE_DIR/pytorch_model.bin\n","\n","# %env BERT_BASE_PYTORCH_FILE=chinese_L-4_H-256_A-4_pytorch.tgz\n","# !tar cvzf $BERT_BASE_PYTORCH_FILE $BERT_BASE_DIR/bert_config.json $BERT_BASE_DIR/pytorch_model.bin $BERT_BASE_DIR/vocab.txt"],"metadata":{"id":"kHOfTcBBfl9s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#BERT-Base"],"metadata":{"id":"mrpPxopPSPJi"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code_pretrain\n","!wget https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/chinese_L-12_H-768_A-12_MODEL-B2.tgz -P /content/drive/MyDrive/Code_pretrain\n","!tar xvf chinese_L-12_H-768_A-12_MODEL-B2.tgz\n","%env BERT_BASE_DIR=chinese_L-12_H-768_A-12\n"],"metadata":{"id":"t5P07bNDS-ZI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684823689914,"user_tz":-480,"elapsed":131509,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"19a60238-63f8-4524-96e1-c7e4a625401c"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Code_pretrain\n","--2023-05-23 06:32:38--  https://transformers-models.obs.cn-north-4.myhuaweicloud.com/bert/cn/pretrain/chinese_L-12_H-768_A-12_MODEL-B2.tgz\n","Resolving transformers-models.obs.cn-north-4.myhuaweicloud.com (transformers-models.obs.cn-north-4.myhuaweicloud.com)... 121.36.121.226, 121.36.121.227\n","Connecting to transformers-models.obs.cn-north-4.myhuaweicloud.com (transformers-models.obs.cn-north-4.myhuaweicloud.com)|121.36.121.226|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1154662181 (1.1G) [binary/octet-stream]\n","Saving to: ‘/content/drive/MyDrive/Code_pretrain/chinese_L-12_H-768_A-12_MODEL-B2.tgz’\n","\n","chinese_L-12_H-768_ 100%[===================>]   1.08G  13.1MB/s    in 88s     \n","\n","2023-05-23 06:34:07 (12.6 MB/s) - ‘/content/drive/MyDrive/Code_pretrain/chinese_L-12_H-768_A-12_MODEL-B2.tgz’ saved [1154662181/1154662181]\n","\n","model_b2_chinese_L-12_H-768_A-12/\n","model_b2_chinese_L-12_H-768_A-12/model.ckpt-197000.index\n","model_b2_chinese_L-12_H-768_A-12/checkpoint\n","model_b2_chinese_L-12_H-768_A-12/graph.pbtxt\n","model_b2_chinese_L-12_H-768_A-12/.ipynb_checkpoints/\n","model_b2_chinese_L-12_H-768_A-12/bert_config.json\n","model_b2_chinese_L-12_H-768_A-12/model.ckpt-197000.data-00000-of-00001\n","model_b2_chinese_L-12_H-768_A-12/model.ckpt-197000.meta\n","model_b2_chinese_L-12_H-768_A-12/vocab.txt\n","env: BERT_BASE_DIR=chinese_L-12_H-768_A-12\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code_pretrain\n","%env BERT_BASE_DIR=model_b2_chinese_L-12_H-768_A-12\n","!pytorch_transformers bert \\\n","  $BERT_BASE_DIR/bert_model.ckpt \\\n","  $BERT_BASE_DIR/bert_config.json \\\n","  $BERT_BASE_DIR/pytorch_model.bin\n","\n","# %env BERT_BASE_PYTORCH_FILE=chinese_L-4_H-256_A-4_pytorch.tgz\n","# !tar cvzf $BERT_BASE_PYTORCH_FILE $BERT_BASE_DIR/bert_config.json $BERT_BASE_DIR/pytorch_model.bin $BERT_BASE_DIR/vocab.txt"],"metadata":{"id":"go_k7RKJSLqe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684823874231,"user_tz":-480,"elapsed":21502,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"d9606933-2464-4c09-94a4-14e2481e01e5"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Code_pretrain\n","env: BERT_BASE_DIR=model_b2_chinese_L-12_H-768_A-12\n","Building PyTorch model from configuration: {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 24883\n","}\n","\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","INFO:pytorch_transformers.modeling_bert:Converting TensorFlow checkpoint from /content/drive/MyDrive/Code_pretrain/model_b2_chinese_L-12_H-768_A-12/bert_model.ckpt\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/position_embeddings/adam_m with shape [512, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/position_embeddings/adam_v with shape [512, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/token_type_embeddings/adam_m with shape [2, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/token_type_embeddings/adam_v with shape [2, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/word_embeddings with shape [24883, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/word_embeddings/adam_m with shape [24883, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/embeddings/word_embeddings/adam_v with shape [24883, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/bias/adam_m with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/bias/adam_v with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel/adam_m with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel/adam_v with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/kernel/adam_m with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/kernel/adam_v with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/bias/adam_m with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/bias/adam_v with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel/adam_m with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel/adam_v with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/kernel/adam_m with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/kernel/adam_v with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/bias/adam_m with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/bias/adam_v with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel/adam_m with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel/adam_v with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/kernel/adam_m with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/kernel/adam_v with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/bias/adam_m with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/bias/adam_v with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel/adam_m with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel/adam_v with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/kernel/adam_m with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/kernel/adam_v with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/bias/adam_m with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/bias/adam_v with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel/adam_m with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel/adam_v with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/kernel/adam_m with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/kernel/adam_v with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/bias/adam_m with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/bias/adam_v with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel/adam_m with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel/adam_v with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/kernel/adam_m with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/kernel/adam_v with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/bias/adam_m with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/bias/adam_v with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel/adam_m with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel/adam_v with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/kernel/adam_m with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/kernel/adam_v with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/bias/adam_m with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/bias/adam_v with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel/adam_m with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel/adam_v with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/kernel/adam_m with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/kernel/adam_v with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/bias/adam_m with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/bias/adam_v with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel/adam_m with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel/adam_v with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/kernel/adam_m with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/kernel/adam_v with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/bias/adam_m with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/bias/adam_v with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel/adam_m with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel/adam_v with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/kernel/adam_m with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/kernel/adam_v with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/bias/adam_m with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/bias/adam_v with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel/adam_m with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel/adam_v with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/kernel/adam_m with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/kernel/adam_v with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/bias/adam_m with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/bias/adam_v with shape [3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel/adam_m with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel/adam_v with shape [768, 3072]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/kernel/adam_m with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/kernel/adam_v with shape [3072, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/pooler/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/pooler/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/pooler/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/pooler/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight bert/pooler/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/output_bias with shape [24883]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/output_bias/adam_m with shape [24883]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/output_bias/adam_v with shape [24883]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/beta/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/gamma/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/bias with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/bias/adam_m with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/bias/adam_v with shape [768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/kernel/adam_m with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/kernel/adam_v with shape [768, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_bias with shape [2]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_bias/adam_m with shape [2]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_bias/adam_v with shape [2]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_weights/adam_m with shape [2, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_weights/adam_v with shape [2, 768]\n","INFO:pytorch_transformers.modeling_bert:Loading TF weight global_step with shape []\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/embeddings/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/embeddings/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/embeddings/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/embeddings/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/embeddings/position_embeddings/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/embeddings/position_embeddings/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/embeddings/token_type_embeddings/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/embeddings/token_type_embeddings/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/embeddings/word_embeddings/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/embeddings/word_embeddings/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/self/key/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/self/key/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/self/key/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/self/key/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/self/query/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/self/query/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/self/query/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/self/query/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/self/value/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/self/value/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/self/value/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/attention/self/value/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/intermediate/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/intermediate/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/intermediate/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/intermediate/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_0/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/self/key/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/self/key/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/self/key/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/self/key/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/self/query/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/self/query/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/self/query/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/self/query/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/self/value/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/self/value/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/self/value/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/attention/self/value/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/intermediate/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/intermediate/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/intermediate/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/intermediate/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_1/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/self/key/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/self/key/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/self/key/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/self/key/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/self/query/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/self/query/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/self/query/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/self/query/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/self/value/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/self/value/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/self/value/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/attention/self/value/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/intermediate/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/intermediate/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/intermediate/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/intermediate/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_10/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/self/key/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/self/key/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/self/key/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/self/key/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/self/query/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/self/query/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/self/query/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/self/query/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/self/value/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/self/value/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/self/value/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/attention/self/value/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/intermediate/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/intermediate/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/intermediate/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/intermediate/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_11/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/self/key/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/self/key/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/self/key/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/self/key/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/self/query/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/self/query/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/self/query/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/self/query/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/self/value/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/self/value/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/self/value/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/attention/self/value/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/intermediate/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/intermediate/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/intermediate/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/intermediate/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_2/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/self/key/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/self/key/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/self/key/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/self/key/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/self/query/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/self/query/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/self/query/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/self/query/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/self/value/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/self/value/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/self/value/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/attention/self/value/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/intermediate/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/intermediate/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/intermediate/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/intermediate/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_3/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/self/key/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/self/key/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/self/key/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/self/key/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/self/query/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/self/query/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/self/query/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/self/query/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/self/value/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/self/value/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/self/value/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/attention/self/value/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/intermediate/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/intermediate/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/intermediate/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/intermediate/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_4/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/self/key/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/self/key/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/self/key/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/self/key/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/self/query/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/self/query/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/self/query/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/self/query/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/self/value/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/self/value/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/self/value/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/attention/self/value/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/intermediate/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/intermediate/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/intermediate/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/intermediate/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_5/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/self/key/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/self/key/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/self/key/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/self/key/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/self/query/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/self/query/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/self/query/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/self/query/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/self/value/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/self/value/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/self/value/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/attention/self/value/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/intermediate/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/intermediate/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/intermediate/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/intermediate/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_6/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/self/key/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/self/key/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/self/key/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/self/key/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/self/query/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/self/query/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/self/query/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/self/query/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/self/value/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/self/value/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/self/value/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/attention/self/value/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/intermediate/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/intermediate/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/intermediate/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/intermediate/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_7/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/self/key/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/self/key/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/self/key/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/self/key/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/self/query/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/self/query/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/self/query/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/self/query/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/self/value/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/self/value/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/self/value/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/attention/self/value/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/intermediate/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/intermediate/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/intermediate/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/intermediate/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_8/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/self/key/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/self/key/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/self/key/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/self/key/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/self/query/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/self/query/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/self/query/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/self/query/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/self/value/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/self/value/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/self/value/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/attention/self/value/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/intermediate/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/intermediate/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/intermediate/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/intermediate/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/output/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/output/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/output/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/output/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/output/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/output/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/output/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/encoder/layer_9/output/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/pooler/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/pooler/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/pooler/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping bert/pooler/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/predictions/output_bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/predictions/output_bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/predictions/transform/LayerNorm/beta/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/predictions/transform/LayerNorm/beta/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/predictions/transform/LayerNorm/gamma/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/predictions/transform/LayerNorm/gamma/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/predictions/transform/dense/bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/predictions/transform/dense/bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/predictions/transform/dense/kernel/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/predictions/transform/dense/kernel/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/seq_relationship/output_bias/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/seq_relationship/output_bias/adam_v\n","INFO:pytorch_transformers.modeling_bert:Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/seq_relationship/output_weights/adam_m\n","INFO:pytorch_transformers.modeling_bert:Skipping cls/seq_relationship/output_weights/adam_v\n","INFO:pytorch_transformers.modeling_bert:Skipping global_step\n","Save PyTorch model to model_b2_chinese_L-12_H-768_A-12/pytorch_model.bin\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code_pretrain\n","%env BERT_BASE_DIR=roberta_lyanna_L-12_H-768_A-12_cn\n","!cp \"model_b2_chinese_L-12_H-768_A-12\"/vocab.txt $BERT_BASE_DIR/vocab.txt\n","!cp \"model_b2_chinese_L-12_H-768_A-12\"/bert_config.json $BERT_BASE_DIR/config.json\n","!cp \"model_b2_chinese_L-12_H-768_A-12\"/pytorch_model.bin $BERT_BASE_DIR/pytorch_model.bin"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6rT_grBnyRm","executionInfo":{"status":"ok","timestamp":1684823923345,"user_tz":-480,"elapsed":3269,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"01615d24-64ab-4d1f-f3eb-cbfeb33cdd9f"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Code_pretrain\n","env: BERT_BASE_DIR=roberta_lyanna_L-12_H-768_A-12_cn\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code_pretrain\n","!zip -r roberta_lyanna_L-12_H-768_A-12_cn.zip roberta_lyanna_L-12_H-768_A-12_cn\n","!obsutil cp roberta_lyanna_L-12_H-768_A-12_cn.zip obs://transformers-models/bert/cn/pretrain/pt/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MNNeF5qiodu3","executionInfo":{"status":"ok","timestamp":1684824004833,"user_tz":-480,"elapsed":53405,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"c9d47380-f7cc-4b3e-98c9-47f4d8f3af75"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Code_pretrain\n","  adding: roberta_lyanna_L-12_H-768_A-12_cn/ (stored 0%)\n","  adding: roberta_lyanna_L-12_H-768_A-12_cn/vocab.txt (deflated 52%)\n","  adding: roberta_lyanna_L-12_H-768_A-12_cn/config.json (deflated 55%)\n","  adding: roberta_lyanna_L-12_H-768_A-12_cn/pytorch_model.bin (deflated 7%)\n","Start at 2023-05-23 06:39:49.725209827 +0000 UTC\n","\n","\n","Parallel:      5                   Jobs:          5                   \n","Threshold:     50.00MB             PartSize:      auto                \n","VerifyLength:  false               VerifyMd5:     false               \n","CheckpointDir: /root/.obsutil_checkpoint     \n","\n","\u001b[37m\u001b[0m\u001b[37m\u001b[0m\n","Waiting for the uploaded key to be completed on server side.\n","\n","\n","Upload successfully, 373.51MB, n/a, /content/drive/MyDrive/Code_pretrain/roberta_lyanna_L-12_H-768_A-12_cn.zip --> obs://transformers-models/bert/cn/pretrain/pt/roberta_lyanna_L-12_H-768_A-12_cn.zip, cost [11527], status [200], request id [000001884756114AEBCA7D9E7A9FF3CE]\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code_pretrain\n","%env BERT_BASE_DIR=roberta_lyanna_L-12_H-768_A-12_cn\n","!cp $BERT_BASE_DIR/vocab.txt $BERT_BASE_DIR\"_tf\"/vocab.txt\n","!cp $BERT_BASE_DIR/config.json $BERT_BASE_DIR\"_tf\"/bert_config.json\n","!python convert_pt-tf.py --model_name=\"bert-model\"  --pytorch_model_path=$BERT_BASE_DIR --tf_cache_dir=\"roberta_lyarra_L-12_H-768_A-12_cn_tf\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAueEon9Jea1","executionInfo":{"status":"ok","timestamp":1684824100381,"user_tz":-480,"elapsed":41199,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"4145487c-9f9b-4415-c49e-42fcefb10600"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Code_pretrain\n","env: BERT_BASE_DIR=roberta_lyanna_L-12_H-768_A-12_cn\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","2023-05-23 06:41:05.698442: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2023-05-23 06:41:05.780859: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200220000 Hz\n","2023-05-23 06:41:05.784017: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x16098b0 executing computations on platform Host. Devices:\n","2023-05-23 06:41:05.784072: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","2023-05-23 06:41:05.861526: W tensorflow/core/framework/allocator.cc:124] Allocation of 76440576 exceeds 10% of system memory.\n","2023-05-23 06:41:05.964944: W tensorflow/core/framework/allocator.cc:124] Allocation of 76440576 exceeds 10% of system memory.\n","2023-05-23 06:41:06.010203: W tensorflow/core/framework/allocator.cc:124] Allocation of 76440576 exceeds 10% of system memory.\n","2023-05-23 06:41:06.055114: W tensorflow/core/framework/allocator.cc:124] Allocation of 76440576 exceeds 10% of system memory.\n","2023-05-23 06:41:06.105770: W tensorflow/core/framework/allocator.cc:124] Allocation of 76440576 exceeds 10% of system memory.\n","Successfully created bert/embeddings/word_embeddings: True\n","Successfully created bert/embeddings/position_embeddings: True\n","Successfully created bert/embeddings/token_type_embeddings: True\n","Successfully created bert/embeddings/LayerNorm/gamma: True\n","Successfully created bert/embeddings/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_0/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_0/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_0/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_0/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_0/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_0/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_0/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_0/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_0/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_0/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_0/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_0/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_0/output/dense/kernel: True\n","Successfully created bert/encoder/layer_0/output/dense/bias: True\n","Successfully created bert/encoder/layer_0/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_0/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_1/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_1/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_1/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_1/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_1/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_1/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_1/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_1/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_1/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_1/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_1/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_1/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_1/output/dense/kernel: True\n","Successfully created bert/encoder/layer_1/output/dense/bias: True\n","Successfully created bert/encoder/layer_1/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_1/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_2/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_2/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_2/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_2/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_2/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_2/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_2/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_2/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_2/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_2/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_2/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_2/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_2/output/dense/kernel: True\n","Successfully created bert/encoder/layer_2/output/dense/bias: True\n","Successfully created bert/encoder/layer_2/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_2/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_3/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_3/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_3/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_3/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_3/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_3/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_3/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_3/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_3/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_3/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_3/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_3/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_3/output/dense/kernel: True\n","Successfully created bert/encoder/layer_3/output/dense/bias: True\n","Successfully created bert/encoder/layer_3/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_3/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_4/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_4/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_4/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_4/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_4/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_4/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_4/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_4/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_4/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_4/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_4/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_4/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_4/output/dense/kernel: True\n","Successfully created bert/encoder/layer_4/output/dense/bias: True\n","Successfully created bert/encoder/layer_4/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_4/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_5/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_5/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_5/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_5/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_5/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_5/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_5/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_5/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_5/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_5/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_5/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_5/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_5/output/dense/kernel: True\n","Successfully created bert/encoder/layer_5/output/dense/bias: True\n","Successfully created bert/encoder/layer_5/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_5/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_6/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_6/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_6/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_6/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_6/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_6/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_6/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_6/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_6/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_6/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_6/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_6/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_6/output/dense/kernel: True\n","Successfully created bert/encoder/layer_6/output/dense/bias: True\n","Successfully created bert/encoder/layer_6/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_6/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_7/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_7/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_7/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_7/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_7/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_7/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_7/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_7/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_7/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_7/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_7/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_7/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_7/output/dense/kernel: True\n","Successfully created bert/encoder/layer_7/output/dense/bias: True\n","Successfully created bert/encoder/layer_7/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_7/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_8/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_8/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_8/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_8/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_8/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_8/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_8/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_8/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_8/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_8/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_8/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_8/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_8/output/dense/kernel: True\n","Successfully created bert/encoder/layer_8/output/dense/bias: True\n","Successfully created bert/encoder/layer_8/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_8/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_9/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_9/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_9/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_9/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_9/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_9/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_9/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_9/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_9/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_9/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_9/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_9/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_9/output/dense/kernel: True\n","Successfully created bert/encoder/layer_9/output/dense/bias: True\n","Successfully created bert/encoder/layer_9/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_9/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_10/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_10/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_10/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_10/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_10/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_10/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_10/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_10/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_10/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_10/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_10/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_10/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_10/output/dense/kernel: True\n","Successfully created bert/encoder/layer_10/output/dense/bias: True\n","Successfully created bert/encoder/layer_10/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_10/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_11/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_11/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_11/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_11/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_11/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_11/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_11/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_11/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_11/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_11/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_11/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_11/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_11/output/dense/kernel: True\n","Successfully created bert/encoder/layer_11/output/dense/bias: True\n","Successfully created bert/encoder/layer_11/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_11/output/LayerNorm/beta: True\n","Successfully created bert/pooler/dense/kernel: True\n","Successfully created bert/pooler/dense/bias: True\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code_pretrain\n","!zip -r roberta_lyanna_L-12_H-768_A-12_cn_tf.zip roberta_lyanna_L-12_H-768_A-12_cn_tf\n","!obsutil cp roberta_lyanna_L-12_H-768_A-12_cn_tf.zip obs://transformers-models/bert/cn/pretrain/tf1/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TmdAx90GSnN5","executionInfo":{"status":"ok","timestamp":1684824263338,"user_tz":-480,"elapsed":56954,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"53eb3c32-14e3-41d5-c60f-60ba5cc73dee"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Code_pretrain\n","  adding: roberta_lyanna_L-12_H-768_A-12_cn_tf/ (stored 0%)\n","  adding: roberta_lyanna_L-12_H-768_A-12_cn_tf/vocab.txt (deflated 52%)\n","  adding: roberta_lyanna_L-12_H-768_A-12_cn_tf/bert_config.json (deflated 55%)\n","  adding: roberta_lyanna_L-12_H-768_A-12_cn_tf/bert_model.ckpt.data-00000-of-00001 (deflated 7%)\n","  adding: roberta_lyanna_L-12_H-768_A-12_cn_tf/bert_model.ckpt.index (deflated 68%)\n","  adding: roberta_lyanna_L-12_H-768_A-12_cn_tf/checkpoint (deflated 43%)\n","  adding: roberta_lyanna_L-12_H-768_A-12_cn_tf/bert_model.ckpt.meta (deflated 92%)\n","Start at 2023-05-23 06:44:02.282723533 +0000 UTC\n","\n","\n","Parallel:      5                   Jobs:          5                   \n","Threshold:     50.00MB             PartSize:      auto                \n","VerifyLength:  false               VerifyMd5:     false               \n","CheckpointDir: /root/.obsutil_checkpoint     \n","\n","\u001b[37m\u001b[0m\u001b[37m\u001b[0m\n","Waiting for the uploaded key to be completed on server side.\n","\n","\n","Upload successfully, 371.36MB, n/a, /content/drive/MyDrive/Code_pretrain/roberta_lyanna_L-12_H-768_A-12_cn_tf.zip --> obs://transformers-models/bert/cn/pretrain/tf1/roberta_lyanna_L-12_H-768_A-12_cn_tf.zip, cost [17400], status [200], request id [00000188475A0225D36B42247C306DDB]\n"]}]},{"cell_type":"code","source":["!pip install  prettytable==2.5.0"],"metadata":{"id":"xmdlSlBUWoOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%env BERT_BASE_DIR=roberta_lyanna_L-12_H-768_A-12_cn\n","#!cp $BERT_BASE_DIR/bert_config.json $BERT_BASE_DIR/config.json\n","!python count.py $BERT_BASE_DIR"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCnF_rTUWXbM","executionInfo":{"status":"ok","timestamp":1684824328140,"user_tz":-480,"elapsed":8453,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"2ae5f6e3-a7e9-4dff-e815-e9c3bb75e05d"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["env: BERT_BASE_DIR=roberta_lyanna_L-12_H-768_A-12_cn\n","TOTAL SIZE:105.151488M\n","+----------------------------------------------------+------------+\n","|                      Modules                       | Parameters |\n","+----------------------------------------------------+------------+\n","|         embeddings.word_embeddings.weight          |  19110144  |\n","|       embeddings.position_embeddings.weight        |   393216   |\n","|      embeddings.token_type_embeddings.weight       |    1536    |\n","|            embeddings.LayerNorm.weight             |    768     |\n","|             embeddings.LayerNorm.bias              |    768     |\n","|    encoder.layer.0.attention.self.query.weight     |   589824   |\n","|     encoder.layer.0.attention.self.query.bias      |    768     |\n","|     encoder.layer.0.attention.self.key.weight      |   589824   |\n","|      encoder.layer.0.attention.self.key.bias       |    768     |\n","|    encoder.layer.0.attention.self.value.weight     |   589824   |\n","|     encoder.layer.0.attention.self.value.bias      |    768     |\n","|   encoder.layer.0.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.0.attention.output.dense.bias     |    768     |\n","| encoder.layer.0.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.0.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.0.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.0.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.0.output.dense.weight         |  2359296   |\n","|         encoder.layer.0.output.dense.bias          |    768     |\n","|      encoder.layer.0.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.0.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.1.attention.self.query.weight     |   589824   |\n","|     encoder.layer.1.attention.self.query.bias      |    768     |\n","|     encoder.layer.1.attention.self.key.weight      |   589824   |\n","|      encoder.layer.1.attention.self.key.bias       |    768     |\n","|    encoder.layer.1.attention.self.value.weight     |   589824   |\n","|     encoder.layer.1.attention.self.value.bias      |    768     |\n","|   encoder.layer.1.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.1.attention.output.dense.bias     |    768     |\n","| encoder.layer.1.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.1.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.1.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.1.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.1.output.dense.weight         |  2359296   |\n","|         encoder.layer.1.output.dense.bias          |    768     |\n","|      encoder.layer.1.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.1.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.2.attention.self.query.weight     |   589824   |\n","|     encoder.layer.2.attention.self.query.bias      |    768     |\n","|     encoder.layer.2.attention.self.key.weight      |   589824   |\n","|      encoder.layer.2.attention.self.key.bias       |    768     |\n","|    encoder.layer.2.attention.self.value.weight     |   589824   |\n","|     encoder.layer.2.attention.self.value.bias      |    768     |\n","|   encoder.layer.2.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.2.attention.output.dense.bias     |    768     |\n","| encoder.layer.2.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.2.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.2.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.2.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.2.output.dense.weight         |  2359296   |\n","|         encoder.layer.2.output.dense.bias          |    768     |\n","|      encoder.layer.2.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.2.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.3.attention.self.query.weight     |   589824   |\n","|     encoder.layer.3.attention.self.query.bias      |    768     |\n","|     encoder.layer.3.attention.self.key.weight      |   589824   |\n","|      encoder.layer.3.attention.self.key.bias       |    768     |\n","|    encoder.layer.3.attention.self.value.weight     |   589824   |\n","|     encoder.layer.3.attention.self.value.bias      |    768     |\n","|   encoder.layer.3.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.3.attention.output.dense.bias     |    768     |\n","| encoder.layer.3.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.3.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.3.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.3.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.3.output.dense.weight         |  2359296   |\n","|         encoder.layer.3.output.dense.bias          |    768     |\n","|      encoder.layer.3.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.3.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.4.attention.self.query.weight     |   589824   |\n","|     encoder.layer.4.attention.self.query.bias      |    768     |\n","|     encoder.layer.4.attention.self.key.weight      |   589824   |\n","|      encoder.layer.4.attention.self.key.bias       |    768     |\n","|    encoder.layer.4.attention.self.value.weight     |   589824   |\n","|     encoder.layer.4.attention.self.value.bias      |    768     |\n","|   encoder.layer.4.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.4.attention.output.dense.bias     |    768     |\n","| encoder.layer.4.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.4.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.4.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.4.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.4.output.dense.weight         |  2359296   |\n","|         encoder.layer.4.output.dense.bias          |    768     |\n","|      encoder.layer.4.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.4.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.5.attention.self.query.weight     |   589824   |\n","|     encoder.layer.5.attention.self.query.bias      |    768     |\n","|     encoder.layer.5.attention.self.key.weight      |   589824   |\n","|      encoder.layer.5.attention.self.key.bias       |    768     |\n","|    encoder.layer.5.attention.self.value.weight     |   589824   |\n","|     encoder.layer.5.attention.self.value.bias      |    768     |\n","|   encoder.layer.5.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.5.attention.output.dense.bias     |    768     |\n","| encoder.layer.5.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.5.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.5.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.5.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.5.output.dense.weight         |  2359296   |\n","|         encoder.layer.5.output.dense.bias          |    768     |\n","|      encoder.layer.5.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.5.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.6.attention.self.query.weight     |   589824   |\n","|     encoder.layer.6.attention.self.query.bias      |    768     |\n","|     encoder.layer.6.attention.self.key.weight      |   589824   |\n","|      encoder.layer.6.attention.self.key.bias       |    768     |\n","|    encoder.layer.6.attention.self.value.weight     |   589824   |\n","|     encoder.layer.6.attention.self.value.bias      |    768     |\n","|   encoder.layer.6.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.6.attention.output.dense.bias     |    768     |\n","| encoder.layer.6.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.6.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.6.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.6.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.6.output.dense.weight         |  2359296   |\n","|         encoder.layer.6.output.dense.bias          |    768     |\n","|      encoder.layer.6.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.6.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.7.attention.self.query.weight     |   589824   |\n","|     encoder.layer.7.attention.self.query.bias      |    768     |\n","|     encoder.layer.7.attention.self.key.weight      |   589824   |\n","|      encoder.layer.7.attention.self.key.bias       |    768     |\n","|    encoder.layer.7.attention.self.value.weight     |   589824   |\n","|     encoder.layer.7.attention.self.value.bias      |    768     |\n","|   encoder.layer.7.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.7.attention.output.dense.bias     |    768     |\n","| encoder.layer.7.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.7.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.7.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.7.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.7.output.dense.weight         |  2359296   |\n","|         encoder.layer.7.output.dense.bias          |    768     |\n","|      encoder.layer.7.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.7.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.8.attention.self.query.weight     |   589824   |\n","|     encoder.layer.8.attention.self.query.bias      |    768     |\n","|     encoder.layer.8.attention.self.key.weight      |   589824   |\n","|      encoder.layer.8.attention.self.key.bias       |    768     |\n","|    encoder.layer.8.attention.self.value.weight     |   589824   |\n","|     encoder.layer.8.attention.self.value.bias      |    768     |\n","|   encoder.layer.8.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.8.attention.output.dense.bias     |    768     |\n","| encoder.layer.8.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.8.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.8.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.8.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.8.output.dense.weight         |  2359296   |\n","|         encoder.layer.8.output.dense.bias          |    768     |\n","|      encoder.layer.8.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.8.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.9.attention.self.query.weight     |   589824   |\n","|     encoder.layer.9.attention.self.query.bias      |    768     |\n","|     encoder.layer.9.attention.self.key.weight      |   589824   |\n","|      encoder.layer.9.attention.self.key.bias       |    768     |\n","|    encoder.layer.9.attention.self.value.weight     |   589824   |\n","|     encoder.layer.9.attention.self.value.bias      |    768     |\n","|   encoder.layer.9.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.9.attention.output.dense.bias     |    768     |\n","| encoder.layer.9.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.9.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.9.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.9.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.9.output.dense.weight         |  2359296   |\n","|         encoder.layer.9.output.dense.bias          |    768     |\n","|      encoder.layer.9.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.9.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.10.attention.self.query.weight    |   589824   |\n","|     encoder.layer.10.attention.self.query.bias     |    768     |\n","|     encoder.layer.10.attention.self.key.weight     |   589824   |\n","|      encoder.layer.10.attention.self.key.bias      |    768     |\n","|    encoder.layer.10.attention.self.value.weight    |   589824   |\n","|     encoder.layer.10.attention.self.value.bias     |    768     |\n","|   encoder.layer.10.attention.output.dense.weight   |   589824   |\n","|    encoder.layer.10.attention.output.dense.bias    |    768     |\n","| encoder.layer.10.attention.output.LayerNorm.weight |    768     |\n","|  encoder.layer.10.attention.output.LayerNorm.bias  |    768     |\n","|     encoder.layer.10.intermediate.dense.weight     |  2359296   |\n","|      encoder.layer.10.intermediate.dense.bias      |    3072    |\n","|        encoder.layer.10.output.dense.weight        |  2359296   |\n","|         encoder.layer.10.output.dense.bias         |    768     |\n","|      encoder.layer.10.output.LayerNorm.weight      |    768     |\n","|       encoder.layer.10.output.LayerNorm.bias       |    768     |\n","|    encoder.layer.11.attention.self.query.weight    |   589824   |\n","|     encoder.layer.11.attention.self.query.bias     |    768     |\n","|     encoder.layer.11.attention.self.key.weight     |   589824   |\n","|      encoder.layer.11.attention.self.key.bias      |    768     |\n","|    encoder.layer.11.attention.self.value.weight    |   589824   |\n","|     encoder.layer.11.attention.self.value.bias     |    768     |\n","|   encoder.layer.11.attention.output.dense.weight   |   589824   |\n","|    encoder.layer.11.attention.output.dense.bias    |    768     |\n","| encoder.layer.11.attention.output.LayerNorm.weight |    768     |\n","|  encoder.layer.11.attention.output.LayerNorm.bias  |    768     |\n","|     encoder.layer.11.intermediate.dense.weight     |  2359296   |\n","|      encoder.layer.11.intermediate.dense.bias      |    3072    |\n","|        encoder.layer.11.output.dense.weight        |  2359296   |\n","|         encoder.layer.11.output.dense.bias         |    768     |\n","|      encoder.layer.11.output.LayerNorm.weight      |    768     |\n","|       encoder.layer.11.output.LayerNorm.bias       |    768     |\n","|                pooler.dense.weight                 |   589824   |\n","|                 pooler.dense.bias                  |    768     |\n","+----------------------------------------------------+------------+\n","Total Trainable Params: 105151488\n"]}]},{"cell_type":"markdown","source":["#BERT-Night-King\n"],"metadata":{"id":"KrQTeSHWcsmq"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code_analysis\n","%env BERT_BASE_DIR=roberta_night-king_L-36_H-1024_A-16_cn\n","#!python convert_pt-tf.py --model_name=\"bert-model\" --pytorch_model_path=$BERT_BASE_DIR --tf_cache_dir=$BERT_BASE_DIR\"_tf\"\n","!cp $BERT_BASE_DIR/vocab.txt $BERT_BASE_DIR\"_tf\"/vocab.txt\n","!cp $BERT_BASE_DIR/config.json $BERT_BASE_DIR\"_tf\"/bert_config.json\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FousVKWpcwMN","executionInfo":{"status":"ok","timestamp":1684727390351,"user_tz":-480,"elapsed":465327,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"7fd0045d-15c0-4146-cef1-efab8765e481"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Code_analysis\n","env: BERT_BASE_DIR=roberta_night-king_L-36_H-1024_A-16_cn\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","2023-05-22 03:42:46.115692: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2023-05-22 03:42:46.122157: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200205000 Hz\n","2023-05-22 03:42:46.122399: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x15886a0 executing computations on platform Host. Devices:\n","2023-05-22 03:42:46.122434: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","2023-05-22 03:42:46.145910: W tensorflow/core/framework/allocator.cc:124] Allocation of 86540288 exceeds 10% of system memory.\n","2023-05-22 03:42:46.432355: W tensorflow/core/framework/allocator.cc:124] Allocation of 86540288 exceeds 10% of system memory.\n","2023-05-22 03:42:46.522251: W tensorflow/core/framework/allocator.cc:124] Allocation of 86540288 exceeds 10% of system memory.\n","2023-05-22 03:42:46.579272: W tensorflow/core/framework/allocator.cc:124] Allocation of 86540288 exceeds 10% of system memory.\n","2023-05-22 03:42:46.634688: W tensorflow/core/framework/allocator.cc:124] Allocation of 86540288 exceeds 10% of system memory.\n","Successfully created bert/embeddings/word_embeddings: True\n","Successfully created bert/embeddings/position_embeddings: True\n","Successfully created bert/embeddings/token_type_embeddings: True\n","Successfully created bert/embeddings/LayerNorm/gamma: True\n","Successfully created bert/embeddings/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_0/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_0/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_0/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_0/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_0/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_0/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_0/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_0/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_0/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_0/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_0/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_0/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_0/output/dense/kernel: True\n","Successfully created bert/encoder/layer_0/output/dense/bias: True\n","Successfully created bert/encoder/layer_0/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_0/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_1/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_1/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_1/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_1/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_1/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_1/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_1/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_1/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_1/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_1/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_1/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_1/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_1/output/dense/kernel: True\n","Successfully created bert/encoder/layer_1/output/dense/bias: True\n","Successfully created bert/encoder/layer_1/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_1/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_2/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_2/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_2/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_2/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_2/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_2/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_2/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_2/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_2/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_2/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_2/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_2/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_2/output/dense/kernel: True\n","Successfully created bert/encoder/layer_2/output/dense/bias: True\n","Successfully created bert/encoder/layer_2/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_2/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_3/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_3/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_3/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_3/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_3/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_3/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_3/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_3/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_3/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_3/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_3/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_3/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_3/output/dense/kernel: True\n","Successfully created bert/encoder/layer_3/output/dense/bias: True\n","Successfully created bert/encoder/layer_3/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_3/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_4/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_4/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_4/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_4/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_4/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_4/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_4/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_4/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_4/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_4/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_4/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_4/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_4/output/dense/kernel: True\n","Successfully created bert/encoder/layer_4/output/dense/bias: True\n","Successfully created bert/encoder/layer_4/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_4/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_5/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_5/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_5/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_5/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_5/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_5/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_5/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_5/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_5/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_5/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_5/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_5/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_5/output/dense/kernel: True\n","Successfully created bert/encoder/layer_5/output/dense/bias: True\n","Successfully created bert/encoder/layer_5/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_5/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_6/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_6/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_6/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_6/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_6/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_6/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_6/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_6/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_6/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_6/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_6/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_6/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_6/output/dense/kernel: True\n","Successfully created bert/encoder/layer_6/output/dense/bias: True\n","Successfully created bert/encoder/layer_6/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_6/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_7/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_7/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_7/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_7/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_7/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_7/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_7/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_7/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_7/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_7/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_7/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_7/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_7/output/dense/kernel: True\n","Successfully created bert/encoder/layer_7/output/dense/bias: True\n","Successfully created bert/encoder/layer_7/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_7/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_8/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_8/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_8/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_8/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_8/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_8/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_8/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_8/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_8/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_8/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_8/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_8/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_8/output/dense/kernel: True\n","Successfully created bert/encoder/layer_8/output/dense/bias: True\n","Successfully created bert/encoder/layer_8/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_8/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_9/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_9/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_9/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_9/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_9/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_9/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_9/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_9/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_9/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_9/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_9/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_9/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_9/output/dense/kernel: True\n","Successfully created bert/encoder/layer_9/output/dense/bias: True\n","Successfully created bert/encoder/layer_9/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_9/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_10/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_10/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_10/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_10/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_10/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_10/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_10/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_10/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_10/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_10/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_10/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_10/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_10/output/dense/kernel: True\n","Successfully created bert/encoder/layer_10/output/dense/bias: True\n","Successfully created bert/encoder/layer_10/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_10/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_11/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_11/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_11/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_11/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_11/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_11/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_11/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_11/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_11/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_11/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_11/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_11/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_11/output/dense/kernel: True\n","Successfully created bert/encoder/layer_11/output/dense/bias: True\n","Successfully created bert/encoder/layer_11/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_11/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_12/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_12/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_12/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_12/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_12/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_12/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_12/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_12/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_12/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_12/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_12/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_12/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_12/output/dense/kernel: True\n","Successfully created bert/encoder/layer_12/output/dense/bias: True\n","Successfully created bert/encoder/layer_12/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_12/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_13/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_13/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_13/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_13/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_13/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_13/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_13/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_13/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_13/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_13/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_13/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_13/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_13/output/dense/kernel: True\n","Successfully created bert/encoder/layer_13/output/dense/bias: True\n","Successfully created bert/encoder/layer_13/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_13/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_14/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_14/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_14/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_14/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_14/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_14/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_14/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_14/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_14/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_14/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_14/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_14/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_14/output/dense/kernel: True\n","Successfully created bert/encoder/layer_14/output/dense/bias: True\n","Successfully created bert/encoder/layer_14/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_14/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_15/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_15/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_15/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_15/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_15/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_15/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_15/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_15/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_15/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_15/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_15/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_15/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_15/output/dense/kernel: True\n","Successfully created bert/encoder/layer_15/output/dense/bias: True\n","Successfully created bert/encoder/layer_15/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_15/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_16/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_16/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_16/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_16/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_16/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_16/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_16/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_16/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_16/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_16/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_16/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_16/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_16/output/dense/kernel: True\n","Successfully created bert/encoder/layer_16/output/dense/bias: True\n","Successfully created bert/encoder/layer_16/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_16/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_17/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_17/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_17/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_17/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_17/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_17/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_17/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_17/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_17/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_17/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_17/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_17/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_17/output/dense/kernel: True\n","Successfully created bert/encoder/layer_17/output/dense/bias: True\n","Successfully created bert/encoder/layer_17/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_17/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_18/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_18/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_18/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_18/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_18/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_18/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_18/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_18/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_18/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_18/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_18/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_18/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_18/output/dense/kernel: True\n","Successfully created bert/encoder/layer_18/output/dense/bias: True\n","Successfully created bert/encoder/layer_18/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_18/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_19/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_19/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_19/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_19/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_19/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_19/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_19/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_19/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_19/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_19/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_19/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_19/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_19/output/dense/kernel: True\n","Successfully created bert/encoder/layer_19/output/dense/bias: True\n","Successfully created bert/encoder/layer_19/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_19/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_20/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_20/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_20/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_20/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_20/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_20/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_20/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_20/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_20/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_20/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_20/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_20/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_20/output/dense/kernel: True\n","Successfully created bert/encoder/layer_20/output/dense/bias: True\n","Successfully created bert/encoder/layer_20/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_20/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_21/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_21/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_21/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_21/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_21/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_21/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_21/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_21/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_21/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_21/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_21/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_21/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_21/output/dense/kernel: True\n","Successfully created bert/encoder/layer_21/output/dense/bias: True\n","Successfully created bert/encoder/layer_21/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_21/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_22/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_22/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_22/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_22/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_22/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_22/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_22/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_22/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_22/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_22/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_22/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_22/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_22/output/dense/kernel: True\n","Successfully created bert/encoder/layer_22/output/dense/bias: True\n","Successfully created bert/encoder/layer_22/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_22/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_23/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_23/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_23/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_23/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_23/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_23/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_23/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_23/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_23/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_23/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_23/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_23/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_23/output/dense/kernel: True\n","Successfully created bert/encoder/layer_23/output/dense/bias: True\n","Successfully created bert/encoder/layer_23/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_23/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_24/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_24/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_24/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_24/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_24/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_24/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_24/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_24/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_24/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_24/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_24/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_24/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_24/output/dense/kernel: True\n","Successfully created bert/encoder/layer_24/output/dense/bias: True\n","Successfully created bert/encoder/layer_24/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_24/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_25/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_25/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_25/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_25/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_25/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_25/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_25/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_25/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_25/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_25/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_25/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_25/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_25/output/dense/kernel: True\n","Successfully created bert/encoder/layer_25/output/dense/bias: True\n","Successfully created bert/encoder/layer_25/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_25/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_26/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_26/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_26/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_26/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_26/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_26/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_26/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_26/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_26/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_26/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_26/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_26/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_26/output/dense/kernel: True\n","Successfully created bert/encoder/layer_26/output/dense/bias: True\n","Successfully created bert/encoder/layer_26/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_26/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_27/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_27/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_27/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_27/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_27/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_27/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_27/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_27/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_27/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_27/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_27/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_27/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_27/output/dense/kernel: True\n","Successfully created bert/encoder/layer_27/output/dense/bias: True\n","Successfully created bert/encoder/layer_27/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_27/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_28/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_28/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_28/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_28/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_28/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_28/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_28/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_28/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_28/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_28/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_28/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_28/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_28/output/dense/kernel: True\n","Successfully created bert/encoder/layer_28/output/dense/bias: True\n","Successfully created bert/encoder/layer_28/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_28/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_29/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_29/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_29/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_29/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_29/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_29/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_29/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_29/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_29/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_29/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_29/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_29/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_29/output/dense/kernel: True\n","Successfully created bert/encoder/layer_29/output/dense/bias: True\n","Successfully created bert/encoder/layer_29/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_29/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_30/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_30/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_30/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_30/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_30/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_30/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_30/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_30/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_30/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_30/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_30/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_30/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_30/output/dense/kernel: True\n","Successfully created bert/encoder/layer_30/output/dense/bias: True\n","Successfully created bert/encoder/layer_30/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_30/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_31/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_31/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_31/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_31/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_31/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_31/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_31/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_31/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_31/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_31/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_31/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_31/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_31/output/dense/kernel: True\n","Successfully created bert/encoder/layer_31/output/dense/bias: True\n","Successfully created bert/encoder/layer_31/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_31/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_32/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_32/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_32/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_32/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_32/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_32/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_32/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_32/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_32/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_32/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_32/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_32/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_32/output/dense/kernel: True\n","Successfully created bert/encoder/layer_32/output/dense/bias: True\n","Successfully created bert/encoder/layer_32/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_32/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_33/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_33/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_33/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_33/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_33/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_33/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_33/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_33/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_33/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_33/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_33/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_33/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_33/output/dense/kernel: True\n","Successfully created bert/encoder/layer_33/output/dense/bias: True\n","Successfully created bert/encoder/layer_33/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_33/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_34/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_34/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_34/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_34/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_34/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_34/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_34/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_34/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_34/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_34/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_34/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_34/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_34/output/dense/kernel: True\n","Successfully created bert/encoder/layer_34/output/dense/bias: True\n","Successfully created bert/encoder/layer_34/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_34/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_35/attention/self/query/kernel: True\n","Successfully created bert/encoder/layer_35/attention/self/query/bias: True\n","Successfully created bert/encoder/layer_35/attention/self/key/kernel: True\n","Successfully created bert/encoder/layer_35/attention/self/key/bias: True\n","Successfully created bert/encoder/layer_35/attention/self/value/kernel: True\n","Successfully created bert/encoder/layer_35/attention/self/value/bias: True\n","Successfully created bert/encoder/layer_35/attention/output/dense/kernel: True\n","Successfully created bert/encoder/layer_35/attention/output/dense/bias: True\n","Successfully created bert/encoder/layer_35/attention/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_35/attention/output/LayerNorm/beta: True\n","Successfully created bert/encoder/layer_35/intermediate/dense/kernel: True\n","Successfully created bert/encoder/layer_35/intermediate/dense/bias: True\n","Successfully created bert/encoder/layer_35/output/dense/kernel: True\n","Successfully created bert/encoder/layer_35/output/dense/bias: True\n","Successfully created bert/encoder/layer_35/output/LayerNorm/gamma: True\n","Successfully created bert/encoder/layer_35/output/LayerNorm/beta: True\n","Successfully created bert/pooler/dense/kernel: True\n","Successfully created bert/pooler/dense/bias: True\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Code_analysis\n","# !zip -r roberta_lyarra_L-12_H-768_A-12_cn_tf.zip roberta_lyarra_L-12_H-768_A-12_cn_tf/\n","!obsutil cp roberta_lyarra_L-12_H-768_A-12_cn_tf.zip obs://transformers-models/bert/cn/pretrain/tf1/"],"metadata":{"id":"FOoECV7am6Ac","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684822042109,"user_tz":-480,"elapsed":14519,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"2065e990-43a5-4333-f0bf-e9a4c0a7d83f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Code_analysis\n","Start at 2023-05-23 06:07:07.980160167 +0000 UTC\n","\n","\n","Parallel:      5                   Jobs:          5                   \n","Threshold:     50.00MB             PartSize:      auto                \n","VerifyLength:  false               VerifyMd5:     false               \n","CheckpointDir: /root/.obsutil_checkpoint     \n","\n","\u001b[37m\u001b[0m\u001b[37m\u001b[0m\n","Waiting for the uploaded key to be completed on server side.\n","\n","\n","Upload successfully, 362.01MB, n/a, /content/drive/MyDrive/Code_analysis/roberta_lyarra_L-12_H-768_A-12_cn_tf.zip --> obs://transformers-models/bert/cn/pretrain/tf1/roberta_lyarra_L-12_H-768_A-12_cn_tf.zip, cost [10869], status [200], request id [0000018847382119D269229E85C3E49B]\n"]}]},{"cell_type":"markdown","source":["#Parameter Count\n","Count the size of models."],"metadata":{"id":"vrZxJoh8IjJd"}},{"cell_type":"code","source":["%env BERT_BASE_DIR=chinese_L-2_H-128_A-2\n","!cp $BERT_BASE_DIR/bert_config.json $BERT_BASE_DIR/config.json\n","!python count.py $BERT_BASE_DIR"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EMqGirCcI0QB","executionInfo":{"status":"ok","timestamp":1684554445663,"user_tz":-480,"elapsed":3223,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"ff16b32e-c276-4aef-ca71-9e26608dcd66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["env: BERT_BASE_DIR=chinese_L-2_H-128_A-2\n","TOTAL SIZE:3.183488M\n","+---------------------------------------------------+------------+\n","|                      Modules                      | Parameters |\n","+---------------------------------------------------+------------+\n","|         embeddings.word_embeddings.weight         |  2704384   |\n","|       embeddings.position_embeddings.weight       |   65536    |\n","|      embeddings.token_type_embeddings.weight      |    256     |\n","|            embeddings.LayerNorm.weight            |    128     |\n","|             embeddings.LayerNorm.bias             |    128     |\n","|    encoder.layer.0.attention.self.query.weight    |   16384    |\n","|     encoder.layer.0.attention.self.query.bias     |    128     |\n","|     encoder.layer.0.attention.self.key.weight     |   16384    |\n","|      encoder.layer.0.attention.self.key.bias      |    128     |\n","|    encoder.layer.0.attention.self.value.weight    |   16384    |\n","|     encoder.layer.0.attention.self.value.bias     |    128     |\n","|   encoder.layer.0.attention.output.dense.weight   |   16384    |\n","|    encoder.layer.0.attention.output.dense.bias    |    128     |\n","| encoder.layer.0.attention.output.LayerNorm.weight |    128     |\n","|  encoder.layer.0.attention.output.LayerNorm.bias  |    128     |\n","|     encoder.layer.0.intermediate.dense.weight     |   65536    |\n","|      encoder.layer.0.intermediate.dense.bias      |    512     |\n","|        encoder.layer.0.output.dense.weight        |   65536    |\n","|         encoder.layer.0.output.dense.bias         |    128     |\n","|      encoder.layer.0.output.LayerNorm.weight      |    128     |\n","|       encoder.layer.0.output.LayerNorm.bias       |    128     |\n","|    encoder.layer.1.attention.self.query.weight    |   16384    |\n","|     encoder.layer.1.attention.self.query.bias     |    128     |\n","|     encoder.layer.1.attention.self.key.weight     |   16384    |\n","|      encoder.layer.1.attention.self.key.bias      |    128     |\n","|    encoder.layer.1.attention.self.value.weight    |   16384    |\n","|     encoder.layer.1.attention.self.value.bias     |    128     |\n","|   encoder.layer.1.attention.output.dense.weight   |   16384    |\n","|    encoder.layer.1.attention.output.dense.bias    |    128     |\n","| encoder.layer.1.attention.output.LayerNorm.weight |    128     |\n","|  encoder.layer.1.attention.output.LayerNorm.bias  |    128     |\n","|     encoder.layer.1.intermediate.dense.weight     |   65536    |\n","|      encoder.layer.1.intermediate.dense.bias      |    512     |\n","|        encoder.layer.1.output.dense.weight        |   65536    |\n","|         encoder.layer.1.output.dense.bias         |    128     |\n","|      encoder.layer.1.output.LayerNorm.weight      |    128     |\n","|       encoder.layer.1.output.LayerNorm.bias       |    128     |\n","|                pooler.dense.weight                |   16384    |\n","|                 pooler.dense.bias                 |    128     |\n","+---------------------------------------------------+------------+\n","Total Trainable Params: 3183488\n"]}]},{"cell_type":"code","source":["%env BERT_BASE_DIR=chinese_L-12_H-768_A-12\n","!cp $BERT_BASE_DIR/bert_config.json $BERT_BASE_DIR/config.json\n","!python count.py $BERT_BASE_DIR"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IdtqHl03SYFu","executionInfo":{"status":"ok","timestamp":1684554828780,"user_tz":-480,"elapsed":6522,"user":{"displayName":"Brian Shen","userId":"08246478872294528508"}},"outputId":"efccb125-7995-4358-80dd-254f9b6f19ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["env: BERT_BASE_DIR=chinese_L-12_H-768_A-12\n","TOTAL SIZE:102.267648M\n","+----------------------------------------------------+------------+\n","|                      Modules                       | Parameters |\n","+----------------------------------------------------+------------+\n","|         embeddings.word_embeddings.weight          |  16226304  |\n","|       embeddings.position_embeddings.weight        |   393216   |\n","|      embeddings.token_type_embeddings.weight       |    1536    |\n","|            embeddings.LayerNorm.weight             |    768     |\n","|             embeddings.LayerNorm.bias              |    768     |\n","|    encoder.layer.0.attention.self.query.weight     |   589824   |\n","|     encoder.layer.0.attention.self.query.bias      |    768     |\n","|     encoder.layer.0.attention.self.key.weight      |   589824   |\n","|      encoder.layer.0.attention.self.key.bias       |    768     |\n","|    encoder.layer.0.attention.self.value.weight     |   589824   |\n","|     encoder.layer.0.attention.self.value.bias      |    768     |\n","|   encoder.layer.0.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.0.attention.output.dense.bias     |    768     |\n","| encoder.layer.0.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.0.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.0.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.0.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.0.output.dense.weight         |  2359296   |\n","|         encoder.layer.0.output.dense.bias          |    768     |\n","|      encoder.layer.0.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.0.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.1.attention.self.query.weight     |   589824   |\n","|     encoder.layer.1.attention.self.query.bias      |    768     |\n","|     encoder.layer.1.attention.self.key.weight      |   589824   |\n","|      encoder.layer.1.attention.self.key.bias       |    768     |\n","|    encoder.layer.1.attention.self.value.weight     |   589824   |\n","|     encoder.layer.1.attention.self.value.bias      |    768     |\n","|   encoder.layer.1.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.1.attention.output.dense.bias     |    768     |\n","| encoder.layer.1.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.1.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.1.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.1.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.1.output.dense.weight         |  2359296   |\n","|         encoder.layer.1.output.dense.bias          |    768     |\n","|      encoder.layer.1.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.1.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.2.attention.self.query.weight     |   589824   |\n","|     encoder.layer.2.attention.self.query.bias      |    768     |\n","|     encoder.layer.2.attention.self.key.weight      |   589824   |\n","|      encoder.layer.2.attention.self.key.bias       |    768     |\n","|    encoder.layer.2.attention.self.value.weight     |   589824   |\n","|     encoder.layer.2.attention.self.value.bias      |    768     |\n","|   encoder.layer.2.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.2.attention.output.dense.bias     |    768     |\n","| encoder.layer.2.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.2.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.2.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.2.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.2.output.dense.weight         |  2359296   |\n","|         encoder.layer.2.output.dense.bias          |    768     |\n","|      encoder.layer.2.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.2.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.3.attention.self.query.weight     |   589824   |\n","|     encoder.layer.3.attention.self.query.bias      |    768     |\n","|     encoder.layer.3.attention.self.key.weight      |   589824   |\n","|      encoder.layer.3.attention.self.key.bias       |    768     |\n","|    encoder.layer.3.attention.self.value.weight     |   589824   |\n","|     encoder.layer.3.attention.self.value.bias      |    768     |\n","|   encoder.layer.3.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.3.attention.output.dense.bias     |    768     |\n","| encoder.layer.3.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.3.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.3.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.3.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.3.output.dense.weight         |  2359296   |\n","|         encoder.layer.3.output.dense.bias          |    768     |\n","|      encoder.layer.3.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.3.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.4.attention.self.query.weight     |   589824   |\n","|     encoder.layer.4.attention.self.query.bias      |    768     |\n","|     encoder.layer.4.attention.self.key.weight      |   589824   |\n","|      encoder.layer.4.attention.self.key.bias       |    768     |\n","|    encoder.layer.4.attention.self.value.weight     |   589824   |\n","|     encoder.layer.4.attention.self.value.bias      |    768     |\n","|   encoder.layer.4.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.4.attention.output.dense.bias     |    768     |\n","| encoder.layer.4.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.4.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.4.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.4.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.4.output.dense.weight         |  2359296   |\n","|         encoder.layer.4.output.dense.bias          |    768     |\n","|      encoder.layer.4.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.4.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.5.attention.self.query.weight     |   589824   |\n","|     encoder.layer.5.attention.self.query.bias      |    768     |\n","|     encoder.layer.5.attention.self.key.weight      |   589824   |\n","|      encoder.layer.5.attention.self.key.bias       |    768     |\n","|    encoder.layer.5.attention.self.value.weight     |   589824   |\n","|     encoder.layer.5.attention.self.value.bias      |    768     |\n","|   encoder.layer.5.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.5.attention.output.dense.bias     |    768     |\n","| encoder.layer.5.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.5.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.5.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.5.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.5.output.dense.weight         |  2359296   |\n","|         encoder.layer.5.output.dense.bias          |    768     |\n","|      encoder.layer.5.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.5.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.6.attention.self.query.weight     |   589824   |\n","|     encoder.layer.6.attention.self.query.bias      |    768     |\n","|     encoder.layer.6.attention.self.key.weight      |   589824   |\n","|      encoder.layer.6.attention.self.key.bias       |    768     |\n","|    encoder.layer.6.attention.self.value.weight     |   589824   |\n","|     encoder.layer.6.attention.self.value.bias      |    768     |\n","|   encoder.layer.6.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.6.attention.output.dense.bias     |    768     |\n","| encoder.layer.6.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.6.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.6.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.6.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.6.output.dense.weight         |  2359296   |\n","|         encoder.layer.6.output.dense.bias          |    768     |\n","|      encoder.layer.6.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.6.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.7.attention.self.query.weight     |   589824   |\n","|     encoder.layer.7.attention.self.query.bias      |    768     |\n","|     encoder.layer.7.attention.self.key.weight      |   589824   |\n","|      encoder.layer.7.attention.self.key.bias       |    768     |\n","|    encoder.layer.7.attention.self.value.weight     |   589824   |\n","|     encoder.layer.7.attention.self.value.bias      |    768     |\n","|   encoder.layer.7.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.7.attention.output.dense.bias     |    768     |\n","| encoder.layer.7.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.7.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.7.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.7.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.7.output.dense.weight         |  2359296   |\n","|         encoder.layer.7.output.dense.bias          |    768     |\n","|      encoder.layer.7.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.7.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.8.attention.self.query.weight     |   589824   |\n","|     encoder.layer.8.attention.self.query.bias      |    768     |\n","|     encoder.layer.8.attention.self.key.weight      |   589824   |\n","|      encoder.layer.8.attention.self.key.bias       |    768     |\n","|    encoder.layer.8.attention.self.value.weight     |   589824   |\n","|     encoder.layer.8.attention.self.value.bias      |    768     |\n","|   encoder.layer.8.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.8.attention.output.dense.bias     |    768     |\n","| encoder.layer.8.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.8.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.8.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.8.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.8.output.dense.weight         |  2359296   |\n","|         encoder.layer.8.output.dense.bias          |    768     |\n","|      encoder.layer.8.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.8.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.9.attention.self.query.weight     |   589824   |\n","|     encoder.layer.9.attention.self.query.bias      |    768     |\n","|     encoder.layer.9.attention.self.key.weight      |   589824   |\n","|      encoder.layer.9.attention.self.key.bias       |    768     |\n","|    encoder.layer.9.attention.self.value.weight     |   589824   |\n","|     encoder.layer.9.attention.self.value.bias      |    768     |\n","|   encoder.layer.9.attention.output.dense.weight    |   589824   |\n","|    encoder.layer.9.attention.output.dense.bias     |    768     |\n","| encoder.layer.9.attention.output.LayerNorm.weight  |    768     |\n","|  encoder.layer.9.attention.output.LayerNorm.bias   |    768     |\n","|     encoder.layer.9.intermediate.dense.weight      |  2359296   |\n","|      encoder.layer.9.intermediate.dense.bias       |    3072    |\n","|        encoder.layer.9.output.dense.weight         |  2359296   |\n","|         encoder.layer.9.output.dense.bias          |    768     |\n","|      encoder.layer.9.output.LayerNorm.weight       |    768     |\n","|       encoder.layer.9.output.LayerNorm.bias        |    768     |\n","|    encoder.layer.10.attention.self.query.weight    |   589824   |\n","|     encoder.layer.10.attention.self.query.bias     |    768     |\n","|     encoder.layer.10.attention.self.key.weight     |   589824   |\n","|      encoder.layer.10.attention.self.key.bias      |    768     |\n","|    encoder.layer.10.attention.self.value.weight    |   589824   |\n","|     encoder.layer.10.attention.self.value.bias     |    768     |\n","|   encoder.layer.10.attention.output.dense.weight   |   589824   |\n","|    encoder.layer.10.attention.output.dense.bias    |    768     |\n","| encoder.layer.10.attention.output.LayerNorm.weight |    768     |\n","|  encoder.layer.10.attention.output.LayerNorm.bias  |    768     |\n","|     encoder.layer.10.intermediate.dense.weight     |  2359296   |\n","|      encoder.layer.10.intermediate.dense.bias      |    3072    |\n","|        encoder.layer.10.output.dense.weight        |  2359296   |\n","|         encoder.layer.10.output.dense.bias         |    768     |\n","|      encoder.layer.10.output.LayerNorm.weight      |    768     |\n","|       encoder.layer.10.output.LayerNorm.bias       |    768     |\n","|    encoder.layer.11.attention.self.query.weight    |   589824   |\n","|     encoder.layer.11.attention.self.query.bias     |    768     |\n","|     encoder.layer.11.attention.self.key.weight     |   589824   |\n","|      encoder.layer.11.attention.self.key.bias      |    768     |\n","|    encoder.layer.11.attention.self.value.weight    |   589824   |\n","|     encoder.layer.11.attention.self.value.bias     |    768     |\n","|   encoder.layer.11.attention.output.dense.weight   |   589824   |\n","|    encoder.layer.11.attention.output.dense.bias    |    768     |\n","| encoder.layer.11.attention.output.LayerNorm.weight |    768     |\n","|  encoder.layer.11.attention.output.LayerNorm.bias  |    768     |\n","|     encoder.layer.11.intermediate.dense.weight     |  2359296   |\n","|      encoder.layer.11.intermediate.dense.bias      |    3072    |\n","|        encoder.layer.11.output.dense.weight        |  2359296   |\n","|         encoder.layer.11.output.dense.bias         |    768     |\n","|      encoder.layer.11.output.LayerNorm.weight      |    768     |\n","|       encoder.layer.11.output.LayerNorm.bias       |    768     |\n","|                pooler.dense.weight                 |   589824   |\n","|                 pooler.dense.bias                  |    768     |\n","+----------------------------------------------------+------------+\n","Total Trainable Params: 102267648\n"]}]}]}